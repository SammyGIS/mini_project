{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# My School Examination Scrapper:\n",
        "\n",
        "## Two-Layer Scraper Documentation\n",
        "\n",
        "This documentation outlines the two-layer scraper process for extracting WAEC question and solution information in JSON format.\n",
        "\n",
        "## **Layer 1: URL Scraper**\n",
        "The first layer scrapes all question and solution URLs and saves them into a CSV file.\n",
        "\n",
        "### **Required Inputs:**\n",
        "Specify the following parameters:\n",
        "\n",
        "- `subject`: The subject you want to scrape (e.g., `mathematics`).\n",
        "- `start_year`: The starting year for the scraping range (e.g., `1988`).\n",
        "- `end_year`: The ending year for the scraping range (e.g., `2023`).\n",
        "- `exam_type`: The type of exam (e.g., `waec`).\n",
        "- `paper_type`: The type of paper (e.g., `obj`).\n",
        "- `total_pages`: The total number of pages to scrape (e.g., `20`).\n",
        "\n",
        "### **Output:**\n",
        "- A CSV file containing the following columns:\n",
        "  - `id`: Unique identifier for each question.\n",
        "  - `question_url`: The URL linking to the question and solution.\n",
        "\n",
        "### **Code Usage:**\n",
        "Run the first-layer script by setting the required parameters and executing it. The output CSV will be generated with the extracted URLs.\n",
        "\n",
        "---\n",
        "\n",
        "## **Layer 2: Question and Solution Extractor**\n",
        "The second layer takes the CSV file generated by the first layer and extracts detailed question and solution information.\n",
        "\n",
        "### **Required Input:**\n",
        "- The file path to the CSV containing the URLs generated by Layer 1.\n",
        "\n",
        "### **Output:**\n",
        "- A JSON file with the extracted data for each question. Each entry contains:\n",
        "  - `id`: Unique identifier.\n",
        "  - `subject_year`: The subject, year, and exam type.\n",
        "  - `topic_year`: Reserved for additional topic information (if available).\n",
        "  - `question`: The question text.\n",
        "  - `question_diagram`: URL to any associated diagram or image.\n",
        "  - `options`: A dictionary of answer options (e.g., `{ \"A\": \"1.75\", \"B\": \"2\" }`).\n",
        "  - `correct_answer`: The correct option.\n",
        "  - `explanation`: Explanation or solution for the question.\n",
        "  - `url`: The URL of the question page.\n",
        "\n",
        "### **Code Usage:**\n",
        "Run the second-layer script by specifying the path to the CSV file. The script processes the URLs and saves the extracted information in JSON format. Additionally, it ensures the JSON is formatted to support LaTeX and Unicode encoding.\n",
        "\n",
        "---\n",
        "\n",
        "## **General Workflow:**\n",
        "1. **Layer 1:**\n",
        "   - Configure parameters (subject, start and end years, etc.).\n",
        "   - Run the URL scraper script.\n",
        "   - Verify that the CSV file is correctly generated.\n",
        "\n",
        "2. **Layer 2:**\n",
        "   - Provide the CSV file path as input.\n",
        "   - Run the question and solution extractor script.\n",
        "   - Verify that the JSON file is correctly generated and formatted.\n",
        "\n",
        "---\n",
        "\n",
        "## **Best Practices:**\n",
        "- Ensure a stable internet connection to avoid request timeouts.\n",
        "- Verify the output CSV after Layer 1 before proceeding to Layer 2.\n",
        "- For large datasets, consider breaking the scraping process into smaller batches to reduce the risk of errors or interruptions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Input for Layer 1:**\n",
        "```python\n",
        "subject = 'mathematics'\n",
        "start_year = 1988\n",
        "end_year = 2023\n",
        "exam_type = 'waec'\n",
        "paper_type = 'obj'\n",
        "total_pages = 20\n",
        "```\n",
        "\n",
        "### **Example Input for Layer 2:**\n",
        "```python\n",
        "csv_file = 'mathematics_waec_obj_1988-2023.csv'\n"
      ],
      "metadata": {
        "id": "oubpodjo2Ozg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer 1"
      ],
      "metadata": {
        "id": "VDKFdm2w3LkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import json\n",
        "\n",
        "# Function to fetch and parse a single page\n",
        "def fetch_subject_page(subject, exam_year, exam_type, paper_type, page):\n",
        "    \"\"\"\n",
        "    Fetches the content of a single page and parses it with BeautifulSoup.\n",
        "    \"\"\"\n",
        "    url = f'https://myschool.ng/classroom/{subject}?exam_type={exam_type}&exam_year={exam_year}&type={paper_type}&topic=&page={page}'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the page was fetched successfully\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch page {page} for year {exam_year}. Status code: {response.status_code}\")\n",
        "        return None, False\n",
        "\n",
        "    # Parse the page content with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    return soup, True\n",
        "\n",
        "# Function to extract data from a page\n",
        "def extract_data(soup):\n",
        "    \"\"\"\n",
        "    Extracts questions, options, solutions, and exam years from the parsed page content.\n",
        "    \"\"\"\n",
        "    # Extract exam years from the page\n",
        "    exam_year_elements = soup.find_all(class_='ml-2 badge bg-success text-light')\n",
        "    exam_year_texts = [element.text.strip() for element in exam_year_elements]\n",
        "\n",
        "    # Extract solution URLs for the questions\n",
        "    solution_elements = soup.find_all(class_='btn btn-sm btn-outline-danger')\n",
        "    links = [link['href'] for link in solution_elements]\n",
        "\n",
        "    # Extract the Serial Numbers of the Questions\n",
        "    sn_elements = soup.find_all(class_='question_sn bg-danger mr-3')\n",
        "    sn_texts = [element.text.strip() for element in sn_elements]\n",
        "\n",
        "    # Return all extracted data in a dictionary\n",
        "    return {\n",
        "        \"id\": sn_texts,\n",
        "        'question_url': links,\n",
        "        'exam_year': exam_year_texts,\n",
        "    }\n",
        "\n",
        "# Function to scrape a single page and extract data\n",
        "def scrape_page(subject, exam_year, exam_type, paper_type, page):\n",
        "    \"\"\"\n",
        "    Scrapes a single page and extracts the data.\n",
        "    \"\"\"\n",
        "    soup, success = fetch_subject_page(subject, exam_year, exam_type, paper_type, page)\n",
        "    if success and soup:\n",
        "        return extract_data(soup)\n",
        "    return None\n",
        "\n",
        "# Main scraping function using ThreadPoolExecutor for parallel processing\n",
        "def scrape_all_pages_parallel(subject, exam_year, exam_type, paper_type, total_pages=20):\n",
        "    \"\"\"\n",
        "    Scrapes all pages of data for a given subject, exam year, and exam type in parallel.\n",
        "    \"\"\"\n",
        "    # Initialize an empty dictionary to store the scraped data\n",
        "    all_data = {\n",
        "        \"id\": [],\n",
        "        'question_url': [],\n",
        "        'exam_year': []\n",
        "    }\n",
        "\n",
        "    # Using ThreadPoolExecutor to scrape pages in parallel\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = []\n",
        "        # Submit each page to be scraped in parallel\n",
        "        for page in range(1, total_pages + 1):\n",
        "            futures.append(executor.submit(scrape_page, subject, exam_year, exam_type, paper_type, page))\n",
        "\n",
        "        # Collect the results as they complete\n",
        "        for future in as_completed(futures):\n",
        "            data = future.result()\n",
        "            if data:\n",
        "                all_data['id'].extend(data['id'])\n",
        "                all_data['question_url'].extend(data['question_url'])\n",
        "                all_data['exam_year'].extend(data['exam_year'])\n",
        "\n",
        "    # Convert the collected data to a DataFrame\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# Loop through the years from start_year to end_year and scrape data for each year\n",
        "def scrape_for_years(subject, start_year, end_year, exam_type, paper_type, total_pages=20):\n",
        "    \"\"\"\n",
        "    Scrapes data for multiple years in the specified range.\n",
        "    \"\"\"\n",
        "    all_exam_data = pd.DataFrame()\n",
        "\n",
        "    # Loop through each year in the specified range\n",
        "    for exam_year in range(start_year, end_year + 1):\n",
        "        print(f\"Scraping data for year: {exam_year}...\")\n",
        "        # Scrape the data for the current year\n",
        "        examination_df = scrape_all_pages_parallel(subject, exam_year, exam_type, paper_type, total_pages)\n",
        "        # Append the data to the combined DataFrame\n",
        "        all_exam_data = pd.concat([all_exam_data, examination_df], ignore_index=True)\n",
        "\n",
        "    return all_exam_data\n",
        "\n",
        "# Run the scraper and measure execution time\n",
        "if __name__ == \"__main__\":\n",
        "    subject = 'mathematics'\n",
        "    start_year = 2022\n",
        "    end_year = 2023\n",
        "    exam_type = 'waec'\n",
        "    paper_type = 'obj'\n",
        "    total_pages = 20  # You can adjust this number to scrape more or fewer pages (20 is a constant)\n",
        "\n",
        "    # Start the timer to measure the execution time\n",
        "    start_time = datetime.now()  # Start timer\n",
        "\n",
        "    # Scrape data for all years in the specified range\n",
        "    all_examination_df = scrape_for_years(subject, start_year, end_year, exam_type, paper_type, total_pages)\n",
        "\n",
        "    # End the timer and calculate the elapsed time\n",
        "    end_time = datetime.now()  # End timer\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    # Display the DataFrame and runtime\n",
        "    print(f\"Scraping completed in {elapsed_time}.\")\n",
        "\n",
        "    # Save the scraped data to a CSV file\n",
        "    all_examination_df.to_csv(f'{subject}_{exam_type}_{paper_type}_{start_year}-{end_year}_url.csv', index=False)\n",
        "    print(\"URL scraped and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUZZJeuP3J5B",
        "outputId": "b80db7d3-8d3c-436a-d5ea-4e411dde88c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping data for year: 2022...\n",
            "Scraping data for year: 2023...\n",
            "Scraping completed in 0:00:05.247603.\n",
            "URL scraped and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer 2"
      ],
      "metadata": {
        "id": "R15IXaMM3QYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import json\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def fetch_question_page(url):\n",
        "    \"\"\"\n",
        "    Fetches and parses the page content for a given URL.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch URL: {url}. Status code: {response.status_code}\")\n",
        "            return None, False\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        return soup, True\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return None, False\n",
        "\n",
        "def extract_question_data(id, soup, base_url):\n",
        "    \"\"\"\n",
        "    Extracts question data from the given BeautifulSoup object.\n",
        "    \"\"\"\n",
        "    # Initialize default response dictionary\n",
        "    default_response = {\n",
        "        'id': id,\n",
        "        'subject_year': None,\n",
        "        'topic_year': None,\n",
        "        'question': None,\n",
        "        'question_diagram': None,\n",
        "        # 'options': {},\n",
        "        'correct_answer': None,\n",
        "        'incorrect_answers': [],\n",
        "        'explanation': None,\n",
        "        'url': base_url\n",
        "    }\n",
        "\n",
        "    if not soup:\n",
        "        return default_response\n",
        "\n",
        "    try:\n",
        "        # Extract question text\n",
        "        parent_div = soup.find('div', class_='question-desc mb-3')\n",
        "        question_text = None\n",
        "        if parent_div:\n",
        "            p_tags = parent_div.find_all('p')\n",
        "            question_text = ' '.join([p.get_text(strip=True) for p in p_tags]) if p_tags else None\n",
        "\n",
        "        # Extract question image URL\n",
        "        img_tag = parent_div.find('img', class_='img-fluid') if parent_div else None\n",
        "        img_url = urljoin(base_url, img_tag['src']) if img_tag and 'src' in img_tag.attrs else None\n",
        "\n",
        "        # Extract correct answer\n",
        "        correct_answer_tag = soup.find_all(class_=\"text-success mb-3\")\n",
        "        correct_answer_key = correct_answer_tag[0].text.strip('Option')[0] if correct_answer_tag else None\n",
        "\n",
        "        # Extract options\n",
        "        options_dict = {}\n",
        "        options_container = soup.find('ul', class_='list-unstyled')\n",
        "        if options_container:\n",
        "            options = options_container.find_all('li')\n",
        "            for option in options:\n",
        "                key = option.find('strong')\n",
        "                value = option.text.split(\".\")[-1].strip() if option.text else None\n",
        "                if key and value:\n",
        "                    key_text = key.text.strip('.').strip()\n",
        "                    options_dict[key_text] = value\n",
        "\n",
        "        # Calculate incorrect answers\n",
        "        incorrect_answers = []\n",
        "        if correct_answer_key and options_dict:\n",
        "            correct_answer = options_dict.get(correct_answer_key)\n",
        "            incorrect_answers = [value for key, value in options_dict.items()\n",
        "                               if key != correct_answer_key]\n",
        "\n",
        "        # Extract explanation\n",
        "        explanation_texts = []\n",
        "        explanation_tags = soup.find_all(class_=\"mb-4\")\n",
        "        for tag in explanation_tags:\n",
        "            text = tag.get_text(strip=True)\n",
        "            if \"Contributions\" in text:\n",
        "                break\n",
        "            explanation_texts.append(text)\n",
        "        explanation_content = ' '.join(explanation_texts).split(\"Explanation\", 1)[-1].strip() if \"Explanation\" in ' '.join(explanation_texts) else None\n",
        "\n",
        "        # Extract subject and year from URL\n",
        "        match = re.search(r'/classroom/([^/]+).*exam_year=(\\d+).*type=([^&]+)', base_url)\n",
        "        subject_year = f\"{match.group(1).capitalize()} {match.group(3).capitalize()} {match.group(2)}\" if match else None\n",
        "\n",
        "        return {\n",
        "            \"id\": id,\n",
        "            \"subject_year\": subject_year,\n",
        "            \"topic_year\": \"\",\n",
        "            \"question\": question_text,\n",
        "            \"question_diagram\": img_url,\n",
        "            # \"options\": options_dict,\n",
        "            \"correct_answer\": [options_dict.get(correct_answer_key)] if correct_answer_key else None,\n",
        "            \"incorrect_answers\": incorrect_answers,\n",
        "            \"explanation\": f\"Hint: {explanation_content}\" if explanation_content else None,\n",
        "            \"url\": base_url\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting data for ID {id}: {e}\")\n",
        "        return default_response\n",
        "\n",
        "def scrape_urls_in_batches(id_list, urls, batch_size=20):\n",
        "    \"\"\"\n",
        "    Scrapes the given URLs in batches and returns a DataFrame of results.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
        "        future_to_url = {executor.submit(fetch_question_page, url): (id_list[i], url)\n",
        "                        for i, url in enumerate(urls)}\n",
        "\n",
        "        for future in as_completed(future_to_url):\n",
        "            id_, url = future_to_url[future]\n",
        "            try:\n",
        "                soup, success = future.result()\n",
        "                data = extract_question_data(id_, soup, url) if success else None\n",
        "                if data:\n",
        "                    results.append(data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing URL {url}: {e}\")\n",
        "                results.append({\n",
        "                    'id': id_,\n",
        "                    'subject_year': None,\n",
        "                    'topic_year': None,\n",
        "                    'question': None,\n",
        "                    'question_diagram': None,\n",
        "                    # 'options': {},\n",
        "                    'correct_answer': None,\n",
        "                    'incorrect_answers': [],\n",
        "                    'explanation': None,\n",
        "                    'url': url\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def save_to_pretty_json_row_by_row(df, filename):\n",
        "    \"\"\"\n",
        "    Saves each row of the DataFrame as a prettified JSON object to a file\n",
        "    encoded for LaTeX and Unicode compatibility.\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for _, row in df.iterrows():\n",
        "            json.dump(row.to_dict(), f, indent=4, ensure_ascii=False)\n",
        "            f.write('\\n')\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function with proper error handling and configuration.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Configuration\n",
        "        csv_file = '/content/mathematics_waec_obj_2022-2023_url.csv'\n",
        "        batch_size = 20\n",
        "\n",
        "        # Extract metadata from CSV filename\n",
        "        filename_parts = csv_file.split('/')[-1].split('_')\n",
        "        subject = filename_parts[0]\n",
        "        exam_type = filename_parts[1]\n",
        "        paper_type = filename_parts[2]\n",
        "        years = filename_parts[3].split('.')[0]\n",
        "        start_year, end_year = years.split('-')\n",
        "\n",
        "        # Load URLs from the CSV file\n",
        "        question_url_df = pd.read_csv(csv_file)\n",
        "        urls = question_url_df['question_url'].tolist()\n",
        "        ids = question_url_df['id'].tolist()\n",
        "\n",
        "        start_time = datetime.now()\n",
        "        print(f\"Starting scraping at {start_time}\")\n",
        "\n",
        "        # Scrape all data\n",
        "        all_examination_df = scrape_urls_in_batches(ids, urls, batch_size=batch_size)\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Scraping completed in {elapsed_time}.\")\n",
        "        print(f\"Total questions scraped: {len(all_examination_df)}\")\n",
        "\n",
        "        # Save data to JSON\n",
        "        output_filename = f'{subject}_{exam_type}_{paper_type}_{start_year}-{end_year}.json'\n",
        "        save_to_pretty_json_row_by_row(all_examination_df, output_filename)\n",
        "        print(f\"Data successfully saved to {output_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FXqjAQG3Oqo",
        "outputId": "268cc774-b43e-4ddd-db62-224f9878f02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scraping at 2025-01-01 23:13:02.903776\n",
            "Scraping completed in 0:00:07.141608.\n",
            "Total questions scraped: 97\n",
            "Data successfully saved to mathematics_waec_obj_2022-2023.json\n"
          ]
        }
      ]
    }
  ]
}